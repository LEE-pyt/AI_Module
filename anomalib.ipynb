{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 딥러닝 이상 탐지 알고리즘 벤치마킹, 개발 및 배포를 위한 라이브러리\n",
    "\n",
    "> 주의:\n",
    "> 이 노트북은 원래 Kaggle의 @innat에 의해 생성되었습니다. [Kaggle에서 보기](https://www.kaggle.com/code/ipythonx/mvtec-ad-anomaly-detection-with-anomalib-library/notebook).\n",
    "\n",
    "[fhailib]: fhailib는 공개 및 비공개 데이터셋에서 벤치마킹을 위해 최신 이상 탐지 알고리즘을 수집하는 목적을 가진 딥러닝 라이브러리입니다. fhailib는 최근 문헌에 설명된 이상 탐지 알고리즘의 준비된 구현뿐만 아니라, 사용자 정의 모델의 개발과 구현을 용이하게 하는 일련의 도구를 제공합니다. 라이브러리는 이미지 기반 이상 탐지에 중점을 두고 있으며, 알고리즘의 목표는 데이터셋 내의 이상 이미지나 이미지 내의 이상 픽셀 영역을 식별하는 것입니다.\n",
    "\n",
    "라이브러리는 **벤치마킹**을 위해 [`MVTec AD`](https://www.mvtec.com/company/research/datasets/mvtec-ad) (CC BY-NC-SA 4.0), 그리고 사용자 정의 데이터셋 **훈련/추론**을 위한 `folder`를 지원합니다. 이 노트북에서는 `MVTec AD` 칫솔 데이터셋에서 patchcore 모델을 훈련시키고 모델 성능을 평가하는 `fhailib` 훈련을 탐구할 것입니다.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from pytorch_lightning import Trainer\n",
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "from src.anomal_util.config import get_configurable_parameters\n",
    "from src.anomal_util.data import get_datamodule\n",
    "from src.anomal_util.data.utils import read_image\n",
    "from src.anomal_util.deploy import OpenVINOInferencer\n",
    "from src.anomal_util.models import get_model\n",
    "from src.anomal_util.pre_processing.transforms import Denormalize\n",
    "from src.anomal_util.utils.callbacks import LoadModelCallback, get_callbacks\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "current_directory = Path.cwd()\n",
    "os.chdir(current_directory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "현재 fhailib 라이브러리에서 사용할 수 있는 이상 탐지 모델은 2개입니다. 구체적으로는 다음과 같습니다:\n",
    "\n",
    "- [FastFlow](https://arxiv.org/abs/2111.07677)\n",
    "- [Patchcore](https://arxiv.org/pdf/2106.08265.pdf)\n",
    "\n",
    "\n",
    "이 튜토리얼에서는 patchcore 사용할 것입니다. 이제 각 모델의 설정 파일 경로를 해당 폴더에서 가져오겠습니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "구성\n",
    "이 시연에서는 위 목록에서 [Patchcore](https://arxiv.org/pdf/2106.08265.pdf) 모델을 선택할 것입니다. 이제 이 모델의 구성 파일을 빠르게 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:\n",
      "  name: mvtec\n",
      "  format: mvtec\n",
      "  path: ./DATA/EOE_inside/images\n",
      "  task: segmentation\n",
      "  category: EOE_inside\n",
      "  train_batch_size: 32\n",
      "  eval_batch_size: 32\n",
      "  num_workers: 8\n",
      "  image_size: 256 # dimensions to which images are resized (mandatory)\n",
      "  center_crop: 224 # dimensions to which images are center-cropped after resizing (optional)\n",
      "  normalization: imagenet # data distribution to which the images will be normalized: [none, imagenet]\n",
      "  transform_config:\n",
      "    train: null\n",
      "    eval: null\n",
      "  test_split_mode: from_dir # options: [from_dir, synthetic]\n",
      "  test_split_ratio: 0.2 # fraction of train images held out testing (usage depends on test_split_mode)\n",
      "  val_split_mode: same_as_test # options: [same_as_test, from_test, synthetic]\n",
      "  val_split_ratio: 0.5 # fraction of train/test images held out for validation (usage depends on val_split_mode)\n",
      "  tiling:\n",
      "    apply: false\n",
      "    tile_size: null\n",
      "    stride: null\n",
      "    remove_border_count: 0\n",
      "    use_random_tiling: False\n",
      "    random_tile_count: 16\n",
      "\n",
      "model:\n",
      "  name: patchcore\n",
      "  backbone: wide_resnet50_2\n",
      "  pre_trained: true\n",
      "  layers:\n",
      "    - layer2\n",
      "    - layer3\n",
      "  coreset_sampling_ratio: 0.1\n",
      "  num_neighbors: 9\n",
      "  normalization_method: min_max # options: [null, min_max, cdf]\n",
      "\n",
      "metrics:\n",
      "  image:\n",
      "    - F1Score\n",
      "    - AUROC\n",
      "  pixel:\n",
      "    - F1Score\n",
      "    - AUROC\n",
      "  threshold:\n",
      "    method: adaptive #options: [adaptive, manual]\n",
      "    manual_image: null\n",
      "    manual_pixel: null\n",
      "\n",
      "visualization:\n",
      "  show_images: False # show images on the screen\n",
      "  save_images: true # save images to the file system\n",
      "  log_images: true # log images to the available loggers (if any)\n",
      "  image_save_path: null # path to which images will be saved\n",
      "  mode: full # options: [\"full\", \"simple\"]\n",
      "\n",
      "project:\n",
      "  seed: 0\n",
      "  path: ./DATA\n",
      "\n",
      "logging:\n",
      "  logger: [] # options: [comet, tensorboard, wandb, csv] or combinations.\n",
      "  log_graph: false # Logs the model graph to respective logger.\n",
      "\n",
      "optimization:\n",
      "  export_mode: null # options: onnx, openvino\n",
      "\n",
      "# PL Trainer Args. Don't add extra parameter here.\n",
      "trainer:\n",
      "  enable_checkpointing: true\n",
      "  default_root_dir: null\n",
      "  gradient_clip_val: 0\n",
      "  gradient_clip_algorithm: norm\n",
      "  num_nodes: 1\n",
      "  devices: 1\n",
      "  enable_progress_bar: true\n",
      "  overfit_batches: 0.0\n",
      "  track_grad_norm: -1\n",
      "  check_val_every_n_epoch: 1 # Don't validate before extracting features.\n",
      "  fast_dev_run: false\n",
      "  accumulate_grad_batches: 1\n",
      "  max_epochs: 1\n",
      "  min_epochs: null\n",
      "  max_steps: -1\n",
      "  min_steps: null\n",
      "  max_time: null\n",
      "  limit_train_batches: 1.0\n",
      "  limit_val_batches: 1.0\n",
      "  limit_test_batches: 1.0\n",
      "  limit_predict_batches: 1.0\n",
      "  val_check_interval: 1.0 # Don't validate before extracting features.\n",
      "  log_every_n_steps: 50\n",
      "  accelerator: auto # <\"cpu\", \"gpu\", \"tpu\", \"ipu\", \"hpu\", \"auto\">\n",
      "  strategy: null\n",
      "  sync_batchnorm: false\n",
      "  precision: 32\n",
      "  enable_model_summary: true\n",
      "  num_sanity_val_steps: 0\n",
      "  profiler: null\n",
      "  benchmark: false\n",
      "  deterministic: false\n",
      "  reload_dataloaders_every_n_epochs: 0\n",
      "  auto_lr_find: false\n",
      "  replace_sampler_ddp: true\n",
      "  detect_anomaly: false\n",
      "  auto_scale_batch_size: true\n",
      "  plugins: null\n",
      "  move_metrics_to_cpu: false\n",
      "  multiple_trainloader_mode: max_size_cycle\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"patchcore\"  #  'fastflow', 'patchcore'\n",
    "CONFIG_PATH = current_directory / f\"src/anomal_util/models/{MODEL}/config.yaml\"\n",
    "with open(file=CONFIG_PATH, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "구성 파일을 경로에서 읽어서 사전 형태로 반환하는 get_configurable_parameter 함수를 사용할 수 있습니다. Padim 구현에 포함된 기본 구성 파일을 사용하며, 이 파일은 데이터셋의 경로로 ./datasets/MVTec를 사용합니다. 구성을 로드한 후에는 이 경로를 덮어쓸 필요가 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Mytask\\main\\fhailib\\anomal_util\\config\\config.py:243: UserWarning: The seed value is now fixed to 0. Up to v0.3.7, the seed was not fixed when the seed value was set to 0. If you want to use the random seed, please select `None` for the seed value (`null` in the YAML file) or remove the `seed` key from the YAML file.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# pass the config file to model, callbacks and datamodule\n",
    "config = get_configurable_parameters(config_path=CONFIG_PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 데이터셋: MVTec AD\n",
    "\n",
    "**MVTec AD**는 산업 검사에 초점을 맞춘 이상 탐지 방법의 벤치마킹을 위한 데이터셋입니다. 이 데이터셋은 15개의 다른 객체 및 텍스처 카테고리로 나누어진 5000개 이상의 고해상도 이미지를 포함하고 있습니다. 각 카테고리에는 결함이 없는 훈련 이미지 세트와 다양한 종류의 결함이 있는 이미지 및 결함이 없는 이미지의 테스트 세트가 포함되어 있습니다. 데이터셋이 루트 데이터셋 디렉토리에 위치하지 않은 경우, anomalib는 자동으로 데이터셋을 설치합니다.\n",
    "\n",
    "이제 anomalib에 구현된 특정 데이터모듈을 사용하여 MVTec AD 데이터셋을 가져올 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA\\EOE_inside\\images\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m datamodule\u001b[38;5;241m.\u001b[39mprepare_data()  \u001b[38;5;66;03m# Downloads the dataset if it's not in the specified `root` directory\u001b[39;00m\n\u001b[0;32m      3\u001b[0m datamodule\u001b[38;5;241m.\u001b[39msetup()  \u001b[38;5;66;03m# Create train/val/test/prediction sets.\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m i, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(data\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[1;32me:\\conda\\envs\\test\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32me:\\conda\\envs\\test\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1327\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1328\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1331\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32me:\\conda\\envs\\test\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1294\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1290\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1291\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1293\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1294\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1295\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1296\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32me:\\conda\\envs\\test\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[0;32m   1120\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[0;32m   1121\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1129\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[0;32m   1130\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[0;32m   1131\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1132\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[0;32m   1134\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1135\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[1;32me:\\conda\\envs\\test\\lib\\multiprocessing\\queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[0;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[1;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[1;32me:\\conda\\envs\\test\\lib\\multiprocessing\\connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[1;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\conda\\envs\\test\\lib\\multiprocessing\\connection.py:330\u001b[0m, in \u001b[0;36mPipeConnection._poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_got_empty_message \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m    328\u001b[0m             _winapi\u001b[38;5;241m.\u001b[39mPeekNamedPipe(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 330\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32me:\\conda\\envs\\test\\lib\\multiprocessing\\connection.py:879\u001b[0m, in \u001b[0;36mwait\u001b[1;34m(object_list, timeout)\u001b[0m\n\u001b[0;32m    876\u001b[0m                 ready_objects\u001b[38;5;241m.\u001b[39madd(o)\n\u001b[0;32m    877\u001b[0m                 timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 879\u001b[0m     ready_handles \u001b[38;5;241m=\u001b[39m \u001b[43m_exhaustive_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaithandle_to_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m     \u001b[38;5;66;03m# request that overlapped reads stop\u001b[39;00m\n\u001b[0;32m    882\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ov \u001b[38;5;129;01min\u001b[39;00m ov_list:\n",
      "File \u001b[1;32me:\\conda\\envs\\test\\lib\\multiprocessing\\connection.py:811\u001b[0m, in \u001b[0;36m_exhaustive_wait\u001b[1;34m(handles, timeout)\u001b[0m\n\u001b[0;32m    809\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    810\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m L:\n\u001b[1;32m--> 811\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWaitForMultipleObjects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    812\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;241m==\u001b[39m WAIT_TIMEOUT:\n\u001b[0;32m    813\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "datamodule = get_datamodule(config)\n",
    "datamodule.prepare_data()  # Downloads the dataset if it's not in the specified `root` directory\n",
    "datamodule.setup()  # Create train/val/test/prediction sets.\n",
    "\n",
    "i, data = next(enumerate(datamodule.val_dataloader()))\n",
    "print(data.keys())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "입력 이미지와 마스크의 형태를 확인해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[\"image\"].shape, data[\"mask\"].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "이제 검증 세트에서 정상 및 비정상 샘플을 시각화할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image_and_mask(sample: dict[str, Any], index: int) -> Image:\n",
    "    img = ToPILImage()(Denormalize()(sample[\"image\"][index].clone()))\n",
    "    msk = ToPILImage()(sample[\"mask\"][index]).convert(\"RGB\")\n",
    "\n",
    "    return Image.fromarray(np.hstack((np.array(img), np.array(msk))))\n",
    "\n",
    "\n",
    "# Visualize an image with a mask\n",
    "show_image_and_mask(data, index=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 모델과 콜백 준비\n",
    "이제 구성 파일을 원하는 대로 업데이트했습니다. 이제 이를 사용하여 모델 훈련을 시작할 수 있습니다. 여기서는 'datamodule', 'model, 그리고 'callbacks'을 사용하여 모델을 훈련할 것입니다. 콜백은 비필수 로직을 포함하는 독립적인 객체입니다. 이 방식을 통해 ModelLoading, Timer, Metrics, Normalization, Visualization 등 가능한 많은 콜백을 주입할 수 있습니다.\n",
    "\n",
    "훈련과 더불어, OpenVINO를 사용한 추론을 수행하고 싶습니다. 따라서 anomalib가 훈련된 모델을 openvino 형식으로 내보낼 수 있도록 내보내기 구성을 openvino로 설정할 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the export-mode to OpenVINO to create the OpenVINO IR model.\n",
    "config.optimization.export_mode = \"openvino\"\n",
    "\n",
    "# Get the model and callbacks\n",
    "model = get_model(config)\n",
    "callbacks = get_callbacks(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training\n",
    "trainer = Trainer(**config.trainer, callbacks=callbacks)\n",
    "trainer.fit(model=model, datamodule=datamodule)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model from checkpoint before evaluating\n",
    "load_model_callback = LoadModelCallback(weights_path=trainer.checkpoint_callback.best_model_path)\n",
    "trainer.callbacks.insert(0, load_model_callback)\n",
    "test_results = trainer.test(model=model, datamodule=datamodule)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenVINO Inference\n",
    "Now that we trained and tested a model, we could check a single inference result using OpenVINO inferencer object. This will demonstrate how a trained model could be used for inference."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenVINO 추론\n",
    "모델을 훈련하고 테스트한 이후, OpenVINO 추론 객체를 사용하여 단일 추론 결과를 확인할 수 있습니다. 이는 훈련된 모델을 추론에 어떻게 사용할 수 있는지를 보여줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"./DATA/EOE_inside/images/test/scratch/000.png\"\n",
    "\n",
    "image = read_image(path=image_path)\n",
    "plt.imshow(image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenVINO 모델 로드하기\n",
    "\n",
    "기본적으로 출력 파일들은 results 디렉토리에 저장됩니다. OpenVINO 모델이 어디에 저장되어 있는지 확인해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = Path(config[\"project\"][\"path\"])\n",
    "print(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openvino_model_path = output_path / \"result\" / config.time /\"openvino\" / \"model.bin\"\n",
    "metadata = output_path / \"result\" / config.time /\"openvino\" / \"metadata.json\"\n",
    "print(openvino_model_path.exists(), metadata.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferencer = OpenVINOInferencer(\n",
    "    path=openvino_model_path,  # Path to the OpenVINO IR model.\n",
    "    metadata=metadata,  # Path to the metadata file.\n",
    "    device=\"CPU\",  # We would like to run it on an Intel CPU.\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 추론 수행\n",
    "OpenVINO 추론기를 사용하여 이미지를 예측하는 것은 'predict' 메소드를 호출하는 것만큼 간단합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = inferencer.predict(image=image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 predictions는 작업 유형과 관련된 모든 관련 정보를 포함합니다. 예를 들어, 분할 모델의 예측에는 이미지, 이상 맵, 예측 점수, 라벨 또는 마스크가 포함될 수 있습니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결과 표시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions.pred_score, predictions.pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the original image\n",
    "plt.imshow(predictions.image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the raw anomaly maps predicted by the model.\n",
    "plt.imshow(predictions.anomaly_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the heatmaps, on which raw anomaly map is overlayed on the original image.\n",
    "plt.imshow(predictions.heat_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the segmentation mask.\n",
    "plt.imshow(predictions.pred_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the segmentation mask with the original image.\n",
    "plt.imshow(predictions.segmentations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anomalib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae223df28f60859a2f400fae8b3a1034248e0a469f5599fd9a89c32908ed7a84"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
